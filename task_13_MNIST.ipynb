{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_mnist():\n",
        "    \"\"\"Load the MNIST dataset from OpenML.\"\"\"\n",
        "    mnist = fetch_openml('mnist_784', version=1)\n",
        "    X = mnist.data\n",
        "    y = mnist.target.astype(int)\n",
        "    return X, y\n",
        "\n",
        "def preprocess_data(X, y):\n",
        "    \"\"\"Preprocess the data by normalizing and converting to the desired format.\"\"\"\n",
        "    X = X / 255.0  # Normalize the data\n",
        "    X = X.values  # Convert DataFrame to ndarray\n",
        "    X = X.reshape(-1, 784, 1) # Reshape X to be a column vector\n",
        "    y = np.array([vectorized_result(label) for label in y])\n",
        "    return X, y\n",
        "\n",
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j-th position.\"\"\"\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = load_mnist()\n",
        "X, y = preprocess_data(X, y)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "training_data = list(zip(X_train, y_train))  # Do not transpose\n",
        "test_data = list(zip(X_test, y_test))  # Do not transpose\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, sizes):\n",
        "        \"\"\"Initialize the network with random weights and biases.\"\"\"\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Feed the input through the network.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Return the gradient of the cost function for a single training example.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        activation = x\n",
        "        activations = [x]\n",
        "        zs = []\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].T, delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].T)\n",
        "\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        \"\"\"Update the network's weights and biases using gradient descent.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def train(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "        \"\"\"Train the neural network using mini-batch stochastic gradient descent.\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            np.random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k + mini_batch_size] for k in range(0, len(training_data), mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                accuracy = self.evaluate(test_data)\n",
        "                print(f\"Epoch {epoch}: {accuracy:.2f}%\")\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"Evaluate the network's performance on the test data.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n",
        "        correct_results = sum(int(x == y) for (x, y) in test_results)\n",
        "        accuracy = (correct_results / len(test_data)) * 100  # Convert to percentage\n",
        "        return accuracy\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Return the derivative of the cost function.\"\"\"\n",
        "        return (output_activations - y)\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return np.where(z >= 0,\n",
        "                    1.0 / (1.0 + np.exp(-z)),\n",
        "                    np.exp(z) / (np.exp(z) + 1))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"The derivative of the sigmoid function.\"\"\"\n",
        "    sig = sigmoid(z)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "# Initialize and train the neural network\n",
        "network = NeuralNetwork([784, 100, 10])  # Example: 784 input neurons, 100 hidden neurons, 10 output neurons\n",
        "network.train(training_data, epochs=30, mini_batch_size=10, eta=3.0, test_data=test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXocvgcNoMxk",
        "outputId": "7de73ed7-4028-44bd-daee-0be0e8699b10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 74.92%\n",
            "Epoch 1: 76.83%\n",
            "Epoch 2: 85.51%\n",
            "Epoch 3: 86.37%\n",
            "Epoch 4: 86.47%\n",
            "Epoch 5: 86.74%\n",
            "Epoch 6: 87.01%\n",
            "Epoch 7: 86.94%\n",
            "Epoch 8: 87.12%\n",
            "Epoch 9: 87.24%\n",
            "Epoch 10: 87.29%\n",
            "Epoch 11: 87.38%\n",
            "Epoch 12: 87.36%\n",
            "Epoch 13: 87.31%\n",
            "Epoch 14: 87.44%\n",
            "Epoch 15: 87.31%\n",
            "Epoch 16: 87.61%\n",
            "Epoch 17: 87.47%\n",
            "Epoch 18: 87.56%\n",
            "Epoch 19: 87.51%\n",
            "Epoch 20: 87.56%\n",
            "Epoch 21: 87.64%\n",
            "Epoch 22: 87.56%\n",
            "Epoch 23: 87.59%\n",
            "Epoch 24: 87.55%\n",
            "Epoch 25: 87.54%\n",
            "Epoch 26: 87.58%\n",
            "Epoch 27: 87.56%\n",
            "Epoch 28: 87.69%\n",
            "Epoch 29: 87.68%\n"
          ]
        }
      ]
    }
  ]
}