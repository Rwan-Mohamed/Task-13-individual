{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Define the neural network class\n",
        "class Network(object):\n",
        "\n",
        "    def __init__(self, sizes):\n",
        "        \"\"\"The list 'sizes' contains the number of neurons in the respective layers of the network.\n",
        "        For example, if the list is [2, 3, 1], the network will have 3 layers: the first layer with\n",
        "        2 neurons, the second with 3, and the third with 1 neuron.\n",
        "        Biases and weights are initialized using a Gaussian distribution.\n",
        "        The input layer does not have biases.\n",
        "        \"\"\"\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        # Xavier initialization of weights\n",
        "        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Return the output of the network if 'a' is input.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a) + b)\n",
        "        return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "        \"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
        "        if test_data: n_test = len(test_data)\n",
        "        n = len(training_data)\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
        "            else:\n",
        "                print(f\"Epoch {j} complete\")\n",
        "            # Monitor cost function\n",
        "            cost = self.calculate_cost(training_data)\n",
        "            print(f\"Cost after epoch {j}: {cost}\")\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        \"\"\"Update the network's weights and biases by applying gradient descent using backpropagation.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Return a tuple '(nabla_b, nabla_w)' representing the gradient for the cost function.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # Feedforward\n",
        "        activation = x\n",
        "        activations = [x]  # Store all activations layer by layer\n",
        "        zs = []  # Store all z vectors layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # Backpropagate through the layers\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"Return the number of test inputs for which the neural network outputs the correct result.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def calculate_cost(self, data):\n",
        "        \"\"\"Calculate the cost (mean squared error) for the training data.\"\"\"\n",
        "        cost = 0.0\n",
        "        for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            cost += np.linalg.norm(a - y)**2 / 2.0\n",
        "        return cost\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Return the vector of partial derivatives for the output activations.\"\"\"\n",
        "        return (output_activations - y)\n",
        "\n",
        "# Sigmoid and its derivative\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))"
      ],
      "metadata": {
        "id": "2A2E7_-qVUhn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR problem data\n",
        "training_data = [\n",
        "    (np.array([[0], [0]]), np.array([[0]])),\n",
        "    (np.array([[0], [1]]), np.array([[1]])),\n",
        "    (np.array([[1], [0]]), np.array([[1]])),\n",
        "    (np.array([[1], [1]]), np.array([[0]]))\n",
        "]\n",
        "\n",
        "# Initialize the network: input size = 2, hidden layer size = 3, output size = 1\n",
        "net = Network([2, 3, 1])\n",
        "\n",
        "# Train the network with 100 epochs, mini-batch size of 4, and learning rate (eta) of 1.0\n",
        "net.SGD(training_data, epochs=150, mini_batch_size=4, eta=1.0)\n",
        "\n",
        "# Test the network after training\n",
        "for x, y in training_data:\n",
        "    predicted = net.feedforward(x)\n",
        "    print(f\"Input: {x.T} -> Predicted: {predicted}, Actual: {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir0AgWz5VVyn",
        "outputId": "22ff83ad-8280-4ab4-ebe5-93c649c18022"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 complete\n",
            "Cost after epoch 0: 0.7524949484489346\n",
            "Epoch 1 complete\n",
            "Cost after epoch 1: 0.7372818364935586\n",
            "Epoch 2 complete\n",
            "Cost after epoch 2: 0.721292774659092\n",
            "Epoch 3 complete\n",
            "Cost after epoch 3: 0.7046430875942592\n",
            "Epoch 4 complete\n",
            "Cost after epoch 4: 0.687496419588183\n",
            "Epoch 5 complete\n",
            "Cost after epoch 5: 0.6700652546476946\n",
            "Epoch 6 complete\n",
            "Cost after epoch 6: 0.6526055751859434\n",
            "Epoch 7 complete\n",
            "Cost after epoch 7: 0.6354045218052948\n",
            "Epoch 8 complete\n",
            "Cost after epoch 8: 0.6187612710980146\n",
            "Epoch 9 complete\n",
            "Cost after epoch 9: 0.6029631887949076\n",
            "Epoch 10 complete\n",
            "Cost after epoch 10: 0.58826104852317\n",
            "Epoch 11 complete\n",
            "Cost after epoch 11: 0.5748479536224251\n",
            "Epoch 12 complete\n",
            "Cost after epoch 12: 0.5628460170288541\n",
            "Epoch 13 complete\n",
            "Cost after epoch 13: 0.5523029262846423\n",
            "Epoch 14 complete\n",
            "Cost after epoch 14: 0.5431979951421683\n",
            "Epoch 15 complete\n",
            "Cost after epoch 15: 0.535455183757475\n",
            "Epoch 16 complete\n",
            "Cost after epoch 16: 0.5289595553907965\n",
            "Epoch 17 complete\n",
            "Cost after epoch 17: 0.5235738207192124\n",
            "Epoch 18 complete\n",
            "Cost after epoch 18: 0.5191525952486558\n",
            "Epoch 19 complete\n",
            "Cost after epoch 19: 0.5155531986590874\n",
            "Epoch 20 complete\n",
            "Cost after epoch 20: 0.5126428329741091\n",
            "Epoch 21 complete\n",
            "Cost after epoch 21: 0.5103026089633588\n",
            "Epoch 22 complete\n",
            "Cost after epoch 22: 0.5084291621466825\n",
            "Epoch 23 complete\n",
            "Cost after epoch 23: 0.50693461901665\n",
            "Epoch 24 complete\n",
            "Cost after epoch 24: 0.5057455590666252\n",
            "Epoch 25 complete\n",
            "Cost after epoch 25: 0.5048014578078356\n",
            "Epoch 26 complete\n",
            "Cost after epoch 26: 0.5040529419454411\n",
            "Epoch 27 complete\n",
            "Cost after epoch 27: 0.503460062847872\n",
            "Epoch 28 complete\n",
            "Cost after epoch 28: 0.5029907031490046\n",
            "Epoch 29 complete\n",
            "Cost after epoch 29: 0.5026191698824363\n",
            "Epoch 30 complete\n",
            "Cost after epoch 30: 0.5023249891738184\n",
            "Epoch 31 complete\n",
            "Cost after epoch 31: 0.5020918954698805\n",
            "Epoch 32 complete\n",
            "Cost after epoch 32: 0.5019069970365407\n",
            "Epoch 33 complete\n",
            "Cost after epoch 33: 0.5017600949361323\n",
            "Epoch 34 complete\n",
            "Cost after epoch 34: 0.5016431320912127\n",
            "Epoch 35 complete\n",
            "Cost after epoch 35: 0.5015497505345382\n",
            "Epoch 36 complete\n",
            "Cost after epoch 36: 0.5014749374013423\n",
            "Epoch 37 complete\n",
            "Cost after epoch 37: 0.5014147429770835\n",
            "Epoch 38 complete\n",
            "Cost after epoch 38: 0.5013660568066504\n",
            "Epoch 39 complete\n",
            "Cost after epoch 39: 0.5013264303193051\n",
            "Epoch 40 complete\n",
            "Cost after epoch 40: 0.5012939365558768\n",
            "Epoch 41 complete\n",
            "Cost after epoch 41: 0.501267059390179\n",
            "Epoch 42 complete\n",
            "Cost after epoch 42: 0.5012446061361051\n",
            "Epoch 43 complete\n",
            "Cost after epoch 43: 0.5012256386601037\n",
            "Epoch 44 complete\n",
            "Cost after epoch 44: 0.5012094191147576\n",
            "Epoch 45 complete\n",
            "Cost after epoch 45: 0.5011953672108582\n",
            "Epoch 46 complete\n",
            "Cost after epoch 46: 0.5011830265870153\n",
            "Epoch 47 complete\n",
            "Cost after epoch 47: 0.5011720383472088\n",
            "Epoch 48 complete\n",
            "Cost after epoch 48: 0.5011621202429443\n",
            "Epoch 49 complete\n",
            "Cost after epoch 49: 0.5011530502985908\n",
            "Epoch 50 complete\n",
            "Cost after epoch 50: 0.5011446539331117\n",
            "Epoch 51 complete\n",
            "Cost after epoch 51: 0.5011367938325002\n",
            "Epoch 52 complete\n",
            "Cost after epoch 52: 0.5011293619858862\n",
            "Epoch 53 complete\n",
            "Cost after epoch 53: 0.5011222734233378\n",
            "Epoch 54 complete\n",
            "Cost after epoch 54: 0.5011154612918913\n",
            "Epoch 55 complete\n",
            "Cost after epoch 55: 0.5011088729839029\n",
            "Epoch 56 complete\n",
            "Cost after epoch 56: 0.5011024670928608\n",
            "Epoch 57 complete\n",
            "Cost after epoch 57: 0.501096211019818\n",
            "Epoch 58 complete\n",
            "Cost after epoch 58: 0.5010900790913972\n",
            "Epoch 59 complete\n",
            "Cost after epoch 59: 0.5010840510800216\n",
            "Epoch 60 complete\n",
            "Cost after epoch 60: 0.5010781110404033\n",
            "Epoch 61 complete\n",
            "Cost after epoch 61: 0.5010722463946934\n",
            "Epoch 62 complete\n",
            "Cost after epoch 62: 0.5010664472131422\n",
            "Epoch 63 complete\n",
            "Cost after epoch 63: 0.5010607056484877\n",
            "Epoch 64 complete\n",
            "Cost after epoch 64: 0.5010550154912121\n",
            "Epoch 65 complete\n",
            "Cost after epoch 65: 0.501049371819837\n",
            "Epoch 66 complete\n",
            "Cost after epoch 66: 0.5010437707259472\n",
            "Epoch 67 complete\n",
            "Cost after epoch 67: 0.5010382090979684\n",
            "Epoch 68 complete\n",
            "Cost after epoch 68: 0.5010326844511513\n",
            "Epoch 69 complete\n",
            "Cost after epoch 69: 0.5010271947938795\n",
            "Epoch 70 complete\n",
            "Cost after epoch 70: 0.5010217385225454\n",
            "Epoch 71 complete\n",
            "Cost after epoch 71: 0.501016314338887\n",
            "Epoch 72 complete\n",
            "Cost after epoch 72: 0.5010109211849876\n",
            "Epoch 73 complete\n",
            "Cost after epoch 73: 0.501005558192162\n",
            "Epoch 74 complete\n",
            "Cost after epoch 74: 0.5010002246407658\n",
            "Epoch 75 complete\n",
            "Cost after epoch 75: 0.5009949199285899\n",
            "Epoch 76 complete\n",
            "Cost after epoch 76: 0.5009896435460095\n",
            "Epoch 77 complete\n",
            "Cost after epoch 77: 0.5009843950564401\n",
            "Epoch 78 complete\n",
            "Cost after epoch 78: 0.5009791740809725\n",
            "Epoch 79 complete\n",
            "Cost after epoch 79: 0.5009739802862864\n",
            "Epoch 80 complete\n",
            "Cost after epoch 80: 0.500968813375149\n",
            "Epoch 81 complete\n",
            "Cost after epoch 81: 0.500963673078942\n",
            "Epoch 82 complete\n",
            "Cost after epoch 82: 0.5009585591517847\n",
            "Epoch 83 complete\n",
            "Cost after epoch 83: 0.5009534713659122\n",
            "Epoch 84 complete\n",
            "Cost after epoch 84: 0.5009484095080403\n",
            "Epoch 85 complete\n",
            "Cost after epoch 85: 0.5009433733765069\n",
            "Epoch 86 complete\n",
            "Cost after epoch 86: 0.5009383627790214\n",
            "Epoch 87 complete\n",
            "Cost after epoch 87: 0.5009333775308966\n",
            "Epoch 88 complete\n",
            "Cost after epoch 88: 0.5009284174536552\n",
            "Epoch 89 complete\n",
            "Cost after epoch 89: 0.5009234823739337\n",
            "Epoch 90 complete\n",
            "Cost after epoch 90: 0.5009185721226195\n",
            "Epoch 91 complete\n",
            "Cost after epoch 91: 0.5009136865341701\n",
            "Epoch 92 complete\n",
            "Cost after epoch 92: 0.500908825446078\n",
            "Epoch 93 complete\n",
            "Cost after epoch 93: 0.5009039886984473\n",
            "Epoch 94 complete\n",
            "Cost after epoch 94: 0.5008991761336594\n",
            "Epoch 95 complete\n",
            "Cost after epoch 95: 0.5008943875961094\n",
            "Epoch 96 complete\n",
            "Cost after epoch 96: 0.5008896229319969\n",
            "Epoch 97 complete\n",
            "Cost after epoch 97: 0.5008848819891599\n",
            "Epoch 98 complete\n",
            "Cost after epoch 98: 0.5008801646169422\n",
            "Epoch 99 complete\n",
            "Cost after epoch 99: 0.5008754706660893\n",
            "Epoch 100 complete\n",
            "Cost after epoch 100: 0.5008707999886624\n",
            "Epoch 101 complete\n",
            "Cost after epoch 101: 0.5008661524379715\n",
            "Epoch 102 complete\n",
            "Cost after epoch 102: 0.5008615278685199\n",
            "Epoch 103 complete\n",
            "Cost after epoch 103: 0.5008569261359588\n",
            "Epoch 104 complete\n",
            "Cost after epoch 104: 0.5008523470970507\n",
            "Epoch 105 complete\n",
            "Cost after epoch 105: 0.5008477906096395\n",
            "Epoch 106 complete\n",
            "Cost after epoch 106: 0.5008432565326226\n",
            "Epoch 107 complete\n",
            "Cost after epoch 107: 0.5008387447259314\n",
            "Epoch 108 complete\n",
            "Cost after epoch 108: 0.5008342550505109\n",
            "Epoch 109 complete\n",
            "Cost after epoch 109: 0.5008297873683041\n",
            "Epoch 110 complete\n",
            "Cost after epoch 110: 0.500825341542236\n",
            "Epoch 111 complete\n",
            "Cost after epoch 111: 0.5008209174362032\n",
            "Epoch 112 complete\n",
            "Cost after epoch 112: 0.5008165149150591\n",
            "Epoch 113 complete\n",
            "Cost after epoch 113: 0.5008121338446049\n",
            "Epoch 114 complete\n",
            "Cost after epoch 114: 0.5008077740915787\n",
            "Epoch 115 complete\n",
            "Cost after epoch 115: 0.500803435523646\n",
            "Epoch 116 complete\n",
            "Cost after epoch 116: 0.5007991180093909\n",
            "Epoch 117 complete\n",
            "Cost after epoch 117: 0.5007948214183064\n",
            "Epoch 118 complete\n",
            "Cost after epoch 118: 0.5007905456207876\n",
            "Epoch 119 complete\n",
            "Cost after epoch 119: 0.5007862904881217\n",
            "Epoch 120 complete\n",
            "Cost after epoch 120: 0.5007820558924819\n",
            "Epoch 121 complete\n",
            "Cost after epoch 121: 0.500777841706918\n",
            "Epoch 122 complete\n",
            "Cost after epoch 122: 0.5007736478053502\n",
            "Epoch 123 complete\n",
            "Cost after epoch 123: 0.5007694740625604\n",
            "Epoch 124 complete\n",
            "Cost after epoch 124: 0.5007653203541862\n",
            "Epoch 125 complete\n",
            "Cost after epoch 125: 0.5007611865567121\n",
            "Epoch 126 complete\n",
            "Cost after epoch 126: 0.5007570725474639\n",
            "Epoch 127 complete\n",
            "Cost after epoch 127: 0.5007529782046005\n",
            "Epoch 128 complete\n",
            "Cost after epoch 128: 0.5007489034071078\n",
            "Epoch 129 complete\n",
            "Cost after epoch 129: 0.5007448480347907\n",
            "Epoch 130 complete\n",
            "Cost after epoch 130: 0.5007408119682676\n",
            "Epoch 131 complete\n",
            "Cost after epoch 131: 0.5007367950889627\n",
            "Epoch 132 complete\n",
            "Cost after epoch 132: 0.5007327972790995\n",
            "Epoch 133 complete\n",
            "Cost after epoch 133: 0.5007288184216948\n",
            "Epoch 134 complete\n",
            "Cost after epoch 134: 0.5007248584005511\n",
            "Epoch 135 complete\n",
            "Cost after epoch 135: 0.5007209171002508\n",
            "Epoch 136 complete\n",
            "Cost after epoch 136: 0.5007169944061499\n",
            "Epoch 137 complete\n",
            "Cost after epoch 137: 0.5007130902043708\n",
            "Epoch 138 complete\n",
            "Cost after epoch 138: 0.5007092043817973\n",
            "Epoch 139 complete\n",
            "Cost after epoch 139: 0.5007053368260669\n",
            "Epoch 140 complete\n",
            "Cost after epoch 140: 0.5007014874255656\n",
            "Epoch 141 complete\n",
            "Cost after epoch 141: 0.5006976560694216\n",
            "Epoch 142 complete\n",
            "Cost after epoch 142: 0.5006938426474988\n",
            "Epoch 143 complete\n",
            "Cost after epoch 143: 0.5006900470503913\n",
            "Epoch 144 complete\n",
            "Cost after epoch 144: 0.5006862691694172\n",
            "Epoch 145 complete\n",
            "Cost after epoch 145: 0.5006825088966126\n",
            "Epoch 146 complete\n",
            "Cost after epoch 146: 0.5006787661247261\n",
            "Epoch 147 complete\n",
            "Cost after epoch 147: 0.5006750407472121\n",
            "Epoch 148 complete\n",
            "Cost after epoch 148: 0.5006713326582266\n",
            "Epoch 149 complete\n",
            "Cost after epoch 149: 0.5006676417526204\n",
            "Input: [[0 0]] -> Predicted: [[0.49202764]], Actual: [[0]]\n",
            "Input: [[0 1]] -> Predicted: [[0.48418111]], Actual: [[1]]\n",
            "Input: [[1 0]] -> Predicted: [[0.51576343]], Actual: [[1]]\n",
            "Input: [[1 1]] -> Predicted: [[0.50861567]], Actual: [[0]]\n"
          ]
        }
      ]
    }
  ]
}